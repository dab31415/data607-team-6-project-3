{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Start Selenium/Chrome Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api.selenium import Selenium\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set vars\n",
    "all_jobs = []\n",
    "tmp_folder = 'browser_cache/jupyter_notebook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_login = 'https://linkedin.com'\n",
    "url_listings = 'https://www.linkedin.com/jobs/search/?geoId=90000070&keywords=data%20scientist&location=New%20York%20City%20Metropolitan%20Area'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open new session\n",
    "try:\n",
    "    shutil.rmtree(tmp_folder)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "browser = Selenium(tmp_folder, browser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get Job URLs (JS Render)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.get_url(url_login) # complete manual login .. use a VPN and burner account!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_jobs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,101,25):\n",
    "    current_url = url_listings + '&start=' + str(i) # increment page\n",
    "    \n",
    "    browser.get_url(current_url) # load page\n",
    "    time.sleep(5) # allow full page load\n",
    "    \n",
    "    r = browser.get_inner_html('//ul[contains(@class, \"jobs-search-results__list\")]')\n",
    "    jsoup = BeautifulSoup(r, 'html.parser')\n",
    "    jobs = jsoup.find_all('li',class_='jobs-search-results__list-item')\n",
    "    all_jobs.extend(list(jobs)) # add to list all_jobs\n",
    "    \n",
    "    print(current_url)\n",
    "    print(len(all_jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "all_jobs_urls = ['https://linkedin.com/jobs/view/' + re.search('(.*?)jobPosting:(.{10})(.*)',str(x)).group(2) for x in all_jobs]\n",
    "\n",
    "# save\n",
    "df = pd.DataFrame(all_jobs_urls)\n",
    "df.to_csv('data/job_urls.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get Job Listing Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.get_url(url_login) # complete manual login .. use a VPN and burner account!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/job_urls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.columns[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['job_title','company_name','region','applicant_count',\n",
    "        'employment_salary_type_level','company_size_industry',\n",
    "        'date_queried','date_posted','description']\n",
    "\n",
    "df_jobs = pd.DataFrame(columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleanup\n",
    "def clean(x):\n",
    "    x = re.sub('\\n',' ',x)\n",
    "    x = re.sub('\\s{2}',' ',x)\n",
    "    x = x.strip()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(df)):\n",
    "    row = []\n",
    "    current_url = df.iloc[i,0]\n",
    "    \n",
    "    browser.get_url(current_url)\n",
    "    time.sleep(5) # allow full page load\n",
    "    \n",
    "    j = browser.get_inner_html('//body')\n",
    "    jsoup = BeautifulSoup(j, 'html.parser') # make soup from java-rendered page\n",
    "    \n",
    "    # build current row\n",
    "    row.append(clean(jsoup.find('h1').text)) # job title\n",
    "    row.append(clean(jsoup.find('span', class_='jobs-unified-top-card__subtitle-primary-grouping').contents[1].text)) # company_name\n",
    "    row.append(clean(jsoup.find('span', class_='jobs-unified-top-card__subtitle-primary-grouping').contents[3].text)) # region\n",
    "    \n",
    "    if jsoup.find('span', class_='jobs-unified-top-card__applicant-count'):\n",
    "        row.append(clean(jsoup.find('span', class_='jobs-unified-top-card__applicant-count').text)) # applicant_count\n",
    "    else: \n",
    "        row.append(clean(jsoup.find('span', class_='jobs-unified-top-card__bullet').text)) # applicant_count\n",
    "\n",
    "    row.append(clean(jsoup.find_all('div', class_='jobs-unified-top-card__job-insight')[0].text)) # employment_salary_type_level\n",
    "    row.append(clean(jsoup.find_all('div', class_='jobs-unified-top-card__job-insight')[1].text)) # company_size_industry\n",
    "    row.append(datetime.datetime.now().strftime('%Y-%m-%d %H:%M')) # date_queried\n",
    "    \n",
    "    if jsoup.find('span', class_='jobs-unified-top-card__posted-date'):\n",
    "        row.append(clean(jsoup.find('span', class_='jobs-unified-top-card__posted-date').text)) # date_posted\n",
    "    else: \n",
    "        row.append(None) # date_posted\n",
    "                  \n",
    "    row.append(clean(jsoup.find('div',class_='jobs-description-content__text').contents[7].text)) # description\n",
    "    \n",
    "    df_jobs.loc[len(df_jobs)] = row # append row to df_jobs\n",
    "    df_jobs.to_csv('data/job_listings.csv') # quick save\n",
    "    print(len(df_jobs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
